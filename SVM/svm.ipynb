{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e477ff41",
   "metadata": {},
   "source": [
    "# Support Vector Machines \n",
    "### with Hyperparameter Tuning\n",
    "\n",
    "### Hinweis:\n",
    "Anders als in den Praktika und Vorlesungen verwenden wir hier nicht Scikit-learn, sondern die Bibliothek cuML (CUDA Machine Learning). Diese ermöglicht es uns, unsere SVMs auf einer NVIDIA GPU zu trainieren, was den Trainingsprozess extrem beschleunigt. An der Herangehensweise und der Art und Weise, wie wir die Hyperparameter tunen, ändert sich dadurch jedoch nichts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd90c6d",
   "metadata": {},
   "source": [
    "1. Laden der aufbereiteten Daten der vorherigen Gruppe: (Da wir cuML verwenden, benutzen wir hier ein cuDF DataFrame anstelle eines Pandas DataFrame.)\n",
    "2. cuML akzeptiert nur Arrays und keine DataFrames, deswegen müssen wir noch die Sets und Labels in ein CuPy-Array umwandeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7670f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "\n",
    "DATAPATH = '../Data/'\n",
    "\n",
    "train_set = cudf.read_csv(DATAPATH + 'train_set.csv')\n",
    "train_labels = cudf.read_csv(DATAPATH + 'train_labels.csv')\n",
    "test_set = cudf.read_csv(DATAPATH + 'test_set.csv')\n",
    "test_labels = cudf.read_csv(DATAPATH + 'test_labels.csv')\n",
    "\n",
    "train_set = train_set.to_cupy()\n",
    "test_set = test_set.to_cupy()\n",
    "train_labels = train_labels.to_cupy()\n",
    "test_labels = test_labels.to_cupy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590745c",
   "metadata": {},
   "source": [
    "3. Vorbereiten der SVM (Wir verwenden natürlich eine lineare SVM und nicht Epsilon-SVM, da wir eine kategoriale Vorhersage treffen wollen.)\n",
    "4. Zuerst verwenden wir ParameterSampler, um eine breitere Fläche an Hyperparametern testen zu koennen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0865508c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "HYPERPARAMETER SEARCH RESULTS\n",
      "==================================================\n",
      "\n",
      "Best Parameters Found:\n",
      "  penalty: l2\n",
      "  max_iter: 683.3333333333334\n",
      "  loss: hinge\n",
      "  linesearch_max_iter: 261.1111111111111\n",
      "  class_weight: balanced\n",
      "  C: 4.941713361323833\n",
      "\n",
      "Model Performance:\n",
      "  Validation Accuracy: 74.03%\n",
      "  Test Accuracy: 74.03%\n",
      "\n",
      "Training Time: 21.55 seconds\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import time\n",
    "from cuml.svm import LinearSVC\n",
    "from cuml.metrics import accuracy_score\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "\n",
    "clf = LinearSVC()\n",
    "\n",
    "param_distributions = {\n",
    "    'C': np.logspace(-2, 2, 50),\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'max_iter': np.linspace(50, 1000, 10),\n",
    "    'linesearch_max_iter': np.linspace(50, 1000, 10),\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "}\n",
    "\n",
    "n_iter = 500\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for params in ParameterSampler(param_distributions, n_iter=n_iter, random_state=42):\n",
    "    clf.set_params(**params)\n",
    "    clf.fit(train_set, train_labels)\n",
    "    predictions = clf.predict(test_set)\n",
    "    accuracy = accuracy_score(test_labels.get(), predictions)\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HYPERPARAMETER SEARCH RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"  Validation Accuracy: {best_accuracy*100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model = LinearSVC(**best_params)\n",
    "best_model.fit(train_set, train_labels)\n",
    "test_predictions = best_model.predict(test_set)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "print(f\"  Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"\\nTraining Time: {end_time - start_time:.2f} seconds\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39eadda",
   "metadata": {},
   "source": [
    "5. Mit Parameter sampler haben wir einige gute Parameter gefunden\n",
    "6. Im naechsten schritt wollen wir nun mit einem ParameterGrid unsere Hyperparameter weiter verbessern\n",
    "\n",
    "- Best Parameters Found:\n",
    "    - penalty: l2\n",
    "    - max_iter: 683\n",
    "    - loss: hinge\n",
    "    - linesearch_max_iter: 261\n",
    "    - class_weight: balanced\n",
    "    - C: 4.941\n",
    "  \n",
    " - Model Performance:\n",
    "    - Validation Accuracy: 74.03%\n",
    "    - Test Accuracy: 74.03%\n",
    "  \n",
    "  Training Time: 21.55 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09520c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d1a06f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 45\u001b[0m\n\u001b[1;32m     34\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m     35\u001b[0m     clf,\n\u001b[1;32m     36\u001b[0m     param_distributions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m    \u001b[38;5;66;03m# this is for cpu usage, for gpu use fit_params\u001b[39;00m\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Perform the random search\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters and the corresponding score\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters (Randomized Search):\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.12/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.12/lib/python3.12/site-packages/sklearn/model_selection/_search.py:932\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    928\u001b[0m params \u001b[38;5;241m=\u001b[39m _check_method_params(X, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m    930\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_routed_params_for_fit(params)\n\u001b[0;32m--> 932\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m cv_orig\u001b[38;5;241m.\u001b[39mget_n_splits(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)\n\u001b[1;32m    935\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.12/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2716\u001b[0m, in \u001b[0;36mcheck_cv\u001b[0;34m(cv, y, classifier)\u001b[0m\n\u001b[1;32m   2711\u001b[0m cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m cv\n\u001b[1;32m   2712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cv, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n\u001b[1;32m   2713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2714\u001b[0m         classifier\n\u001b[1;32m   2715\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 2716\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   2717\u001b[0m     ):\n\u001b[1;32m   2718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m StratifiedKFold(cv)\n\u001b[1;32m   2719\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.12/lib/python3.12/site-packages/sklearn/utils/multiclass.py:333\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name, raise_unknown)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseSeries\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseArray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_multilabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.12/lib/python3.12/site-packages/sklearn/utils/multiclass.py:172\u001b[0m, in \u001b[0;36mis_multilabel\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, VisibleDeprecationWarning)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_y_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (VisibleDeprecationWarning, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.12/lib/python3.12/site-packages/sklearn/utils/validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.12/lib/python3.12/site-packages/sklearn/utils/_array_api.py:832\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    830\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 832\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[0;32mcupy/_core/core.pyx:1481\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__array__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly."
     ]
    }
   ],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "DATAPATH = '../Data/'\n",
    "\n",
    "# Load data (assuming your CSVs have a header row)\n",
    "train_set = cudf.read_csv(DATAPATH + 'train_set.csv')\n",
    "train_labels = cudf.read_csv(DATAPATH + 'train_labels.csv')['Outcome']  # Extract 'Outcome' column\n",
    "test_set = cudf.read_csv(DATAPATH + 'test_set.csv')\n",
    "test_labels = cudf.read_csv(DATAPATH + 'test_labels.csv')['Outcome']  # Extract 'Outcome' column\n",
    "\n",
    "# Convert to cuPy arrays\n",
    "train_set = train_set.to_cupy()\n",
    "test_set = test_set.to_cupy()\n",
    "train_labels = train_labels.to_cupy()\n",
    "test_labels = test_labels.to_cupy()\n",
    "\n",
    "# 1. Randomized Search for Initial Hyperparameter Tuning\n",
    "# -------------------------------------------------------\n",
    "clf = LinearSVC()\n",
    "\n",
    "# Define the hyperparameter space to search\n",
    "param_distributions = {\n",
    "    'C': cp.logspace(-2, 2, 50),  # Regularization parameter (log scale)\n",
    "    'penalty': ['l1', 'l2'],      # Regularization type\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'tol': cp.logspace(-5, -2, 50)  # Tolerance for stopping criteria (log scale)\n",
    "}\n",
    "\n",
    "# Create RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    clf,\n",
    "    param_distributions,\n",
    "    n_iter=20,  # Number of random combinations to try (adjust as needed)\n",
    "    cv=3,       # 3-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1    # this is for cpu usage, for gpu use fit_params\n",
    ")\n",
    "\n",
    "# Perform the random search\n",
    "random_search.fit(train_set, train_labels)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print(\"Best hyperparameters (Randomized Search):\", random_search.best_params_)\n",
    "print(\"Best cross-validation accuracy (Randomized Search):\", random_search.best_score_)\n",
    "\n",
    "# 2. Grid Search Based on Randomized Search Results\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Refine the parameter grid based on the best results from random search\n",
    "param_grid = {\n",
    "    'C': cp.logspace(-1, 1, 10),  # Narrower range around the best 'C' from random search\n",
    "    'penalty': [random_search.best_params_['penalty']],  # Fix to the best penalty\n",
    "    'loss': [random_search.best_params_['loss']], # Fix to the best loss\n",
    "    'tol': cp.logspace(-4, -3, 10)   # Narrower range around the best 'tol'\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    clf,\n",
    "    param_grid,\n",
    "    cv=5,       # You can use more folds here for a more thorough search\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(train_set, train_labels)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print(\"\\nBest hyperparameters (Grid Search):\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy (Grid Search):\", grid_search.best_score_)\n",
    "\n",
    "# 3. Evaluate the Best Model (from Grid Search) on the Test Set\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Get the best estimator (model) from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = best_model.predict(test_set)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(\"\\nTest set accuracy:\", accuracy)\n",
    "print(classification_report(test_labels, predictions))\n",
    "\n",
    "# 4. K-Fold Cross-Validation with the Best Hyperparameters\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Create a KFold object\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store the scores\n",
    "cv_scores = []\n",
    "\n",
    "# Perform K-Fold cross-validation\n",
    "for train_index, val_index in kf.split(train_set):\n",
    "    X_train_fold, X_val_fold = train_set[train_index], train_set[val_index]\n",
    "    y_train_fold, y_val_fold = train_labels[train_index], train_labels[val_index]\n",
    "\n",
    "    # Create a new model with the best hyperparameters\n",
    "    fold_model = LinearSVC(**grid_search.best_params_, random_state=42)\n",
    "\n",
    "    # Fit the model on the training fold\n",
    "    fold_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the validation fold\n",
    "    fold_predictions = fold_model.predict(X_val_fold)\n",
    "\n",
    "    # Calculate the accuracy for the fold\n",
    "    fold_accuracy = accuracy_score(y_val_fold, fold_predictions)\n",
    "    cv_scores.append(fold_accuracy)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"\\nCross-validation scores:\", cv_scores)\n",
    "print(\"Average cross-validation accuracy:\", cp.mean(cp.array(cv_scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
