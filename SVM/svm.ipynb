{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e477ff41",
   "metadata": {},
   "source": [
    "# Support Vector Machines \n",
    "### with Hyperparameter Tuning\n",
    "\n",
    "### Hinweis:\n",
    "Anders als in den Praktika und Vorlesungen verwenden wir hier nicht `Scikit-learn`, sondern die Bibliothek `cuML` (CUDA Machine Learning). Diese ermöglicht es uns, unsere SVMs auf einer NVIDIA GPU zu trainieren, was den Trainingsprozess extrem beschleunigt. An der Herangehensweise und der Art, wie wir die Hyperparameter tunen, ändert sich dadurch jedoch nichts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd90c6d",
   "metadata": {},
   "source": [
    "1. Laden der aufbereiteten Daten der vorherigen Gruppe: (Da wir cuML verwenden, benutzen wir hier ein cuDF DataFrame anstelle eines Pandas DataFrame.)\n",
    "2. cuML akzeptiert nur Arrays und keine DataFrames, deswegen müssen wir noch die Sets und Labels in ein CuPy-Array umwandeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7670f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import time\n",
    "from cuml.svm import SVC\n",
    "from cuml.metrics import accuracy_score\n",
    "from sklearn.model_selection import ParameterSampler, ParameterGrid\n",
    "from cuml.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "DATAPATH = '../Data/'\n",
    "MODELPATH = '../Data/Models/SVM/'\n",
    "MODELPATHSKLEARN = '../Data/Models/SVM/SVM_sklearn/'\n",
    "\n",
    "train_set = cudf.read_csv(DATAPATH + 'train_set.csv')\n",
    "train_labels = cudf.read_csv(DATAPATH + 'train_labels.csv')\n",
    "test_set = cudf.read_csv(DATAPATH + 'test_set.csv')\n",
    "test_labels = cudf.read_csv(DATAPATH + 'test_labels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ec5bb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7532467246055603\n"
     ]
    }
   ],
   "source": [
    "clf = SVC()\n",
    "\n",
    "clf.fit(train_set, train_labels)\n",
    "\n",
    "test_pred = clf.predict(test_set)\n",
    "accuracy_score = accuracy_score(test_labels, test_pred)\n",
    "\n",
    "joblib.dump(clf, MODELPATH + 'SVM_no_hyper.pkl')\n",
    "\n",
    "print(f'Accuracy: {accuracy_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590745c",
   "metadata": {},
   "source": [
    "* Ohne Hyperparameter Tuning erhalten wir eine Accuracy von **ca. 0.753** ein wert den wir auf jeden fall noch verbessern wollen.\n",
    "* Als erstes verwenden wir `ParameterSampler`, um eine breitere Fläche an Hyperparametern testen zu koennen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1977591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Data/Models/SVM/SVM_para_sampl.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_distributions = [\n",
    "    {\n",
    "        'kernel': ['linear'],\n",
    "        'C': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True), \n",
    "        'tol': np.logspace(np.log10(0.0000001), np.log10(0.1), 7, endpoint=True),\n",
    "        'max_iter': [1000, 2000, 3000, 4000, 5000],\n",
    "        'class_weight': ['balanced'],\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['poly'],\n",
    "        'C': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True), \n",
    "        'tol': np.logspace(np.log10(0.0000001), np.log10(0.1), 7, endpoint=True),\n",
    "        'gamma': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True),\n",
    "        'degree': [2, 3, 4, 5, 6], # Macht natrülich nur Sinn, wenn degree > 1 da sonst linear\n",
    "        'max_iter': [1000, 2000, 3000, 4000, 5000],\n",
    "        'class_weight': ['balanced'],\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True),\n",
    "        'tol': np.logspace(np.log10(0.0000001), np.log10(0.1), 7, endpoint=True),\n",
    "        'gamma': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True),\n",
    "        'max_iter': [1000, 2000, 3000, 4000, 5000],\n",
    "        'class_weight': ['balanced'],\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['sigmoid'],\n",
    "        'C': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True),\n",
    "        'tol': np.logspace(np.log10(0.0000001), np.log10(0.1), 7, endpoint=True),\n",
    "        'gamma': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True),\n",
    "        'coef0': np.linspace(-1, 1, 10),\n",
    "        'max_iter': [1000, 2000, 3000, 4000, 5000],\n",
    "        'class_weight': ['balanced'],\n",
    "    }\n",
    "]\n",
    "\n",
    "n_iter = 100\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "all_sampled_params = []\n",
    "for params in param_distributions:\n",
    "    sampler = ParameterSampler(params, n_iter=n_iter, random_state=42)\n",
    "    sampled_params = list(sampler)\n",
    "    all_sampled_params.extend(sampled_params)\n",
    "\n",
    "clf = SVC()\n",
    "\n",
    "for params in all_sampled_params:\n",
    "    try:\n",
    "        clf.set_params(**params)\n",
    "        clf.fit(train_set, train_labels)\n",
    "        predictions = clf.predict(test_set)\n",
    "        accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = params\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping invalid parameter combination: {params}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "best_model = SVC(**best_params)\n",
    "best_model.fit(train_set, train_labels) \n",
    "test_predictions = best_model.predict(test_set)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "joblib.dump(best_model, MODELPATH + 'SVM_para_sampl.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9b964d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'tol': np.float64(0.1), 'max_iter': 4000, 'kernel': 'poly', 'gamma': np.float64(0.1), 'degree': 5, 'class_weight': 'balanced', 'C': np.float64(1.0)}\n",
      "Best accuracy: 0.7662337422370911\n",
      "Time taken: 35.68 seconds\n",
      "Test accuracy with best model: 0.701298713684082\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best accuracy: {best_accuracy}\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Test accuracy with best model: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39eadda",
   "metadata": {},
   "source": [
    "### 5. Parameter-Optimierung mit `ParameterSampler`\n",
    "\n",
    "Mithilfe des `ParameterSamplers` konnten wir einige vielversprechende Parameter identifizieren:\n",
    "\n",
    "- Gefundene Parameter:\n",
    "    -  kernel: poly\n",
    "    -  C: 1.0\n",
    "    -  degree: 5\n",
    "    -  tol: 0.1\n",
    "    -  gamma: 0.1\n",
    "    -  max_iter: 4000\n",
    "    -  class_weight: balanced\n",
    "\n",
    "- Modell-Performance:\n",
    "    - Validierungsgenauigkeit: 76.62%\n",
    "    - Testgenauigkeit: 70.13% (Hinweis auf Overfitting)\n",
    "\n",
    "- Trainingszeit: 21.55 Sekunden\n",
    "\n",
    "### 6. Verfeinerung der Hyperparameter mit `ParameterGrid`\n",
    "\n",
    "Im nächsten Schritt werden wir nun ein `ParameterGrid` verwenden, um die Hyperparameter weiter zu optimieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09520c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Data/Models/SVM/SVM_para_grid.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "        'kernel': ['poly'],\n",
    "        'C': [0.01, 0.1, 0.8, 0.9, 1, 1.1, 1.2, 2, 5, 10], \n",
    "        'tol': [0.01, 0.01, 0.1],\n",
    "        'gamma': [0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        'degree': [4, 5, 6],\n",
    "        'max_iter': np.linspace(3900, 4100, 10),\n",
    "        'class_weight': ['balanced'],\n",
    "    },\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    clf.set_params(**params)\n",
    "    clf.fit(train_set, train_labels)\n",
    "    predictions = clf.predict(test_set)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params    \n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "best_model = SVC(**best_params)\n",
    "best_model.fit(train_set, train_labels)\n",
    "test_predictions = best_model.predict(test_set)\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "joblib.dump(best_model, MODELPATH + 'SVM_para_grid.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8831b481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GRID SEARCH RESULTS\n",
      "==================================================\n",
      "\n",
      "Best Parameters Found:\n",
      "  C: 1.2\n",
      "  class_weight: balanced\n",
      "  degree: 4\n",
      "  gamma: 0.07\n",
      "  kernel: poly\n",
      "  max_iter: 3900.0\n",
      "  tol: 0.1\n",
      "\n",
      "Model Performance:\n",
      "  Validation Accuracy: 78.57%\n",
      "  Test Accuracy: 66.88%\n",
      "\n",
      "Training Time: 436.49 seconds\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GRID SEARCH RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"  Validation Accuracy: {best_accuracy*100:.2f}%\")\n",
    "\n",
    "print(f\"  Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"\\nTraining Time: {end_time - start_time:.2f} seconds\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131bbc59",
   "metadata": {},
   "source": [
    "Obwohl wir scheinbar bessere Parameter gefunden haben, hat sich die Test-Genauigkeit sogar verschlechtert! Ein klares Zeichen für Overfitting.\n",
    "\n",
    "- Best Parameters Found:\n",
    "  - C: 1.2\n",
    "  - class_weight: balanced\n",
    "  - degree: 4\n",
    "  - gamma: 0.07\n",
    "  - kernel: poly\n",
    "  - max_iter: 3900.0\n",
    "  - tol: 0.1\n",
    "\n",
    "- Model Performance:\n",
    "  - **Validation Accuracy: 78.57%**\n",
    "  - **Test Accuracy: 66.88%**\n",
    "\n",
    "- Training Time: 406.26 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b5275",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Bisher haben wir verschiedene Methoden des Hyperparameter-Tunings betrachtet, um unsere Modelle zu verbessern. Um das volle Potenzial des Tunings auszuschöpfen und vor allem Overfitting zu vermeiden, benötigen wir jedoch ein weiteres wichtiges Werkzeug: die Cross-Validation (Kreuzvalidierung).\n",
    "\n",
    "Im nächsten Schritt werden wir wieder damit beginnen, die Parameter zufällig zu wählen. Dieses Mal werden wir jedoch zusätzlich die Cross-Validation verwenden.\n",
    "\n",
    "wir benutzen wieder die selbe Parameter Distrobution wie bei `ParameterSampler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8968c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = [\n",
    "    {\n",
    "        'kernel': ['linear'],\n",
    "        'C': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True), \n",
    "        'tol': np.logspace(np.log10(0.0000001), np.log10(0.1), 7, endpoint=True),\n",
    "        'max_iter': [1000, 2000, 3000, 4000, 5000],\n",
    "        'class_weight': ['balanced'],\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['poly'],\n",
    "        'C': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True), \n",
    "        'tol': np.logspace(np.log10(0.0000001), np.log10(0.1), 7, endpoint=True),\n",
    "        'gamma': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True),\n",
    "        'degree': [2, 3, 4, 5, 6], # Macht natrülich nur Sinn, wenn degree > 1 da sonst linear\n",
    "        'max_iter': [1000, 2000, 3000, 4000, 5000],\n",
    "        'class_weight': ['balanced'],\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True),\n",
    "        'tol': np.logspace(np.log10(0.0000001), np.log10(0.1), 7, endpoint=True),\n",
    "        'gamma': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True),\n",
    "        'max_iter': [1000, 2000, 3000, 4000, 5000],\n",
    "        'class_weight': ['balanced'],\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['sigmoid'],\n",
    "        'C': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True),\n",
    "        'tol': np.logspace(np.log10(0.0000001), np.log10(0.1), 7, endpoint=True),\n",
    "        'gamma': np.logspace(np.log10(0.001), np.log10(1000), 7, endpoint=True),\n",
    "        'coef0': np.linspace(-1, 1, 10),\n",
    "        'max_iter': [1000, 2000, 3000, 4000, 5000],\n",
    "        'class_weight': ['balanced'],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b47cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.astype('float32')\n",
    "train_labels = train_labels.astype('float32')\n",
    "test_set = test_set.astype('float32')\n",
    "test_labels = test_labels.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74ddd86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cudf.core.dataframe.DataFrame'>\n",
      "RangeIndex: 800 entries, 0 to 799\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   Outcome  800 non-null    float32\n",
      "dtypes: float32(1)\n",
      "memory usage: 3.1 KB\n"
     ]
    }
   ],
   "source": [
    "train_labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7809daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def randomized_search_with_cv(X, y, param_distributions, n_iter, cv_method='kfold', k=5):\n",
    "    \"\"\"\n",
    "    Performs randomized search with cross-validation for cuML's SVC.\n",
    "\n",
    "    Args:\n",
    "        X: cuDF DataFrame or cuPy array, feature matrix.\n",
    "        y: cuDF Series or cuPy array, target variable.\n",
    "        param_distributions: List of Dictionaries, parameter space to search.\n",
    "        n_iter: Integer, number of parameter settings to sample from each dict.\n",
    "        cv_method: String, 'kfold' or 'stratified' (default: 'kfold').\n",
    "        k: Integer, number of folds for k-fold cross-validation (default: 5).\n",
    "\n",
    "    Returns:\n",
    "        best_params: Dictionary, best hyperparameter combination found.\n",
    "        best_score: Float, best average cross-validation score.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for param_distribution in param_distributions:\n",
    "        # Use ParameterSampler to generate parameter combinations\n",
    "        sampler = ParameterSampler(param_distribution, n_iter=n_iter, random_state=42)\n",
    "        sampled_params_list = list(sampler)\n",
    "\n",
    "        for sampled_params in sampled_params_list:\n",
    "            # 2. Cross-validation\n",
    "            scores = []\n",
    "            if cv_method == 'kfold':\n",
    "                # Implementation for k-fold\n",
    "                fold_size = len(X) // k\n",
    "                for fold in range(k):\n",
    "                    start = fold * fold_size\n",
    "                    end = (fold + 1) * fold_size\n",
    "    \n",
    "                    X_train = cudf.concat([X.iloc[:start], X.iloc[end:]])\n",
    "                    y_train = cudf.concat([y.iloc[:start], y.iloc[end:]])\n",
    "                    X_val = X.iloc[start:end]\n",
    "                    y_val = y.iloc[start:end]\n",
    "    \n",
    "                    model = SVC(**sampled_params)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    score = model.score(X_val, y_val)\n",
    "                    scores.append(score)\n",
    "\n",
    "            elif cv_method == 'stratified':\n",
    "                # Implementation for stratified k-fold\n",
    "                from cuml.model_selection import StratifiedKFold\n",
    "                skf = StratifiedKFold(n_splits=k)\n",
    "    \n",
    "                # Ensure X is a DataFrame (if it's not already)\n",
    "                if not isinstance(X, cudf.DataFrame):\n",
    "                    X = cudf.DataFrame(X)\n",
    "    \n",
    "                for train_index, val_index in skf.split(X, y):\n",
    "                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "                    model = SVC(**sampled_params)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    score = model.score(X_val, y_val)\n",
    "                    scores.append(score)\n",
    "    \n",
    "            elif cv_method == 'loocv':\n",
    "                # Implementation for leave-one-out cv\n",
    "                for i in range(len(X)):\n",
    "                    X_train = X.drop(i)\n",
    "                    y_train = y.drop(i)\n",
    "                    X_val = X.iloc[[i]]\n",
    "                    y_val = y.iloc[[i]]\n",
    "                \n",
    "                    model = SVC(**sampled_params)\n",
    "                    model.fit(X_train, y_train)\n",
    "                    score = model.score(X_val, y_val)\n",
    "                    scores.append(score)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Invalid cv_method. Choose 'kfold' or 'stratified'.\")\n",
    "\n",
    "            avg_score = np.mean(scores)\n",
    "            results.append({'params': sampled_params, 'score': avg_score})\n",
    "\n",
    "    # 3. Find the best result\n",
    "    best_result = max(results, key=lambda x: x['score'])\n",
    "    best_params = best_result['params']\n",
    "    best_score = best_result['score']\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5080b404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W] [20:05:52.244767] SVC with the linear kernel can be much faster using the specialized solver provided by LinearSVC. Consider switching to LinearSVC if tranining takes too long.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "exception occurred! file=/opt/conda/conda-bld/work/cpp/src/svm/kernelcache.cuh line=444: Working set has already been initialized!\nObtained 35 stack frames\n#1 in /home/chris/miniconda3/envs/cuml/lib/python3.12/site-packages/cuml/internals/../../../../libcuml++.so: ML::SVM::KernelCache<float, std::experimental::mdspan<float, std::experimental::extents<int, 18446744073709551615ul, 18446744073709551615ul>, std::experimental::layout_stride, raft::host_device_accessor<std::experimental::default_accessor<float>, (raft::memory_type)2> > >::InitWorkingSet(int const*) +0x3ec [0x7f0d3597144c]\n#2 in /home/chris/miniconda3/envs/cuml/lib/python3.12/site-packages/cuml/internals/../../../../libcuml++.so: void ML::SVM::SmoSolver<float>::Solve<std::experimental::mdspan<float, std::experimental::extents<int, 18446744073709551615ul, 18446744073709551615ul>, std::experimental::layout_stride, raft::host_device_accessor<std::experimental::default_accessor<float>, (raft::memory_type)2> > >(std::experimental::mdspan<float, std::experimental::extents<int, 18446744073709551615ul, 18446744073709551615ul>, std::experimental::layout_stride, raft::host_device_accessor<std::experimental::default_accessor<float>, (raft::memory_type)2> >, int, int, float*, float const*, float**, int*, ML::SVM::SupportStorage<float>*, int**, float*, int, int) +0x43a [0x7f0d3598c42a]\n#3 in /home/chris/miniconda3/envs/cuml/lib/python3.12/site-packages/cuml/internals/../../../../libcuml++.so: void ML::SVM::svcFitX<float, std::experimental::mdspan<float, std::experimental::extents<int, 18446744073709551615ul, 18446744073709551615ul>, std::experimental::layout_stride, raft::host_device_accessor<std::experimental::default_accessor<float>, (raft::memory_type)2> > >(raft::handle_t const&, std::experimental::mdspan<float, std::experimental::extents<int, 18446744073709551615ul, 18446744073709551615ul>, std::experimental::layout_stride, raft::host_device_accessor<std::experimental::default_accessor<float>, (raft::memory_type)2> >, int, int, float*, ML::SVM::SvmParameter const&, raft::distance::kernels::KernelParams&, ML::SVM::SvmModel<float>&, float const*) +0xa39 [0x7f0d3598fe19]\n#4 in /home/chris/miniconda3/envs/cuml/lib/python3.12/site-packages/cuml/internals/../../../../libcuml++.so: void ML::SVM::svcFit<float>(raft::handle_t const&, float*, int, int, float*, ML::SVM::SvmParameter const&, raft::distance::kernels::KernelParams&, ML::SVM::SvmModel<float>&, float const*) +0x4b [0x7f0d3599036b]\n#5 in /home/chris/miniconda3/envs/cuml/lib/python3.12/site-packages/cuml/svm/svc.cpython-312-x86_64-linux-gnu.so(+0x481c7) [0x7f0c886b11c7]\n#6 in /home/chris/miniconda3/envs/cuml/bin/python(+0x113339) [0x561e4e04e339]\n#7 in /home/chris/miniconda3/envs/cuml/bin/python: PyEval_EvalCode +0xa1 [0x561e4e1f5741]\n#8 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2d5ece) [0x561e4e210ece]\n#9 in /home/chris/miniconda3/envs/cuml/bin/python(+0x112f8e) [0x561e4e04df8e]\n#10 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2d099f) [0x561e4e20b99f]\n#11 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2d1c57) [0x561e4e20cc57]\n#12 in /home/chris/miniconda3/envs/cuml/bin/python(+0x113e38) [0x561e4e04ee38]\n#13 in /home/chris/miniconda3/envs/cuml/bin/python(+0x251adc) [0x561e4e18cadc]\n#14 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2515be) [0x561e4e18c5be]\n#15 in /home/chris/miniconda3/envs/cuml/bin/python: _PyObject_Call +0x12b [0x561e4e1701ab]\n#16 in /home/chris/miniconda3/envs/cuml/bin/python(+0x113339) [0x561e4e04e339]\n#17 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2d099f) [0x561e4e20b99f]\n#18 in /home/chris/miniconda3/envs/cuml/lib/python3.12/lib-dynload/_asyncio.cpython-312-x86_64-linux-gnu.so(+0x8274) [0x7f0e5f8de274]\n#19 in /home/chris/miniconda3/envs/cuml/lib/python3.12/lib-dynload/_asyncio.cpython-312-x86_64-linux-gnu.so(+0x8a63) [0x7f0e5f8dea63]\n#20 in /home/chris/miniconda3/envs/cuml/bin/python(+0x222fbc) [0x561e4e15dfbc]\n#21 in /home/chris/miniconda3/envs/cuml/bin/python(+0x34db0c) [0x561e4e288b0c]\n#22 in /home/chris/miniconda3/envs/cuml/bin/python(+0x1c402e) [0x561e4e0ff02e]\n#23 in /home/chris/miniconda3/envs/cuml/bin/python(+0x21940b) [0x561e4e15440b]\n#24 in /home/chris/miniconda3/envs/cuml/bin/python(+0x113339) [0x561e4e04e339]\n#25 in /home/chris/miniconda3/envs/cuml/bin/python: PyEval_EvalCode +0xa1 [0x561e4e1f5741]\n#26 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2d5ece) [0x561e4e210ece]\n#27 in /home/chris/miniconda3/envs/cuml/bin/python(+0x21940b) [0x561e4e15440b]\n#28 in /home/chris/miniconda3/envs/cuml/bin/python: PyObject_Vectorcall +0x2e [0x561e4e1541ae]\n#29 in /home/chris/miniconda3/envs/cuml/bin/python(+0x1126a1) [0x561e4e04d6a1]\n#30 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2eb328) [0x561e4e226328]\n#31 in /home/chris/miniconda3/envs/cuml/bin/python: Py_RunMain +0x3d1 [0x561e4e225ed1]\n#32 in /home/chris/miniconda3/envs/cuml/bin/python: Py_BytesMain +0x37 [0x561e4e1e00c7]\n#33 in /lib/x86_64-linux-gnu/libc.so.6(+0x29d90) [0x7f0e60d95d90]\n#34 in /lib/x86_64-linux-gnu/libc.so.6: __libc_start_main +0x80 [0x7f0e60d95e40]\n#35 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2a4f71) [0x561e4e1dff71]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_params, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_search_with_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkfold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 41\u001b[0m, in \u001b[0;36mrandomized_search_with_cv\u001b[0;34m(X, y, param_distributions, n_iter, cv_method, k)\u001b[0m\n\u001b[1;32m     38\u001b[0m y_val \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[start:end]\n\u001b[1;32m     40\u001b[0m model \u001b[38;5;241m=\u001b[39m SVC(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msampled_params)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(X_val, y_val)\n\u001b[1;32m     43\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "File \u001b[0;32m~/miniconda3/envs/cuml/lib/python3.12/site-packages/cuml/internals/api_decorators.py:188\u001b[0m, in \u001b[0;36m_make_decorator_function.<locals>.decorator_function.<locals>.decorator_closure.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m     set_api_output_dtype(output_dtype)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process_return:\n\u001b[0;32m--> 188\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32msvc.pyx:559\u001b[0m, in \u001b[0;36mcuml.svm.svc.SVC.fit\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msvc.pyx:561\u001b[0m, in \u001b[0;36mcuml.svm.svc.SVC.fit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: exception occurred! file=/opt/conda/conda-bld/work/cpp/src/svm/kernelcache.cuh line=444: Working set has already been initialized!\nObtained 35 stack frames\n#1 in /home/chris/miniconda3/envs/cuml/lib/python3.12/site-packages/cuml/internals/../../../../libcuml++.so: ML::SVM::KernelCache<float, std::experimental::mdspan<float, std::experimental::extents<int, 18446744073709551615ul, 18446744073709551615ul>, std::experimental::layout_stride, raft::host_device_accessor<std::experimental::default_accessor<float>, (raft::memory_type)2> > >::InitWorkingSet(int const*) +0x3ec [0x7f0d3597144c]\n#2 in /home/chris/miniconda3/envs/cuml/lib/python3.12/site-packages/cuml/internals/../../../../libcuml++.so: void ML::SVM::SmoSolver<float>::Solve<std::experimental::mdspan<float, std::experimental::extents<int, 18446744073709551615ul, 18446744073709551615ul>, std::experimental::layout_stride, raft::host_device_accessor<std::experimental::default_accessor<float>, (raft::memory_type)2> > >(std::experimental::mdspan<float, std::experimental::extents<int, 18446744073709551615ul, 18446744073709551615ul>, std::experimental::layout_stride, raft::host_device_accessor<std::experimental::default_accessor<float>, (raft::memory_type)2> >, int, int, float*, float const*, float**, int*, ML::SVM::SupportStorage<float>*, int**, float*, int, int) +0x43a [0x7f0d3598c42a]\n#3 in /home/chris/miniconda3/envs/cuml/lib/python3.12/site-packages/cuml/internals/../../../../libcuml++.so: void ML::SVM::svcFitX<float, std::experimental::mdspan<float, std::experimental::extents<int, 18446744073709551615ul, 18446744073709551615ul>, std::experimental::layout_stride, raft::host_device_accessor<std::experimental::default_accessor<float>, (raft::memory_type)2> > >(raft::handle_t const&, std::experimental::mdspan<float, std::experimental::extents<int, 18446744073709551615ul, 18446744073709551615ul>, std::experimental::layout_stride, raft::host_device_accessor<std::experimental::default_accessor<float>, (raft::memory_type)2> >, int, int, float*, ML::SVM::SvmParameter const&, raft::distance::kernels::KernelParams&, ML::SVM::SvmModel<float>&, float const*) +0xa39 [0x7f0d3598fe19]\n#4 in /home/chris/miniconda3/envs/cuml/lib/python3.12/site-packages/cuml/internals/../../../../libcuml++.so: void ML::SVM::svcFit<float>(raft::handle_t const&, float*, int, int, float*, ML::SVM::SvmParameter const&, raft::distance::kernels::KernelParams&, ML::SVM::SvmModel<float>&, float const*) +0x4b [0x7f0d3599036b]\n#5 in /home/chris/miniconda3/envs/cuml/lib/python3.12/site-packages/cuml/svm/svc.cpython-312-x86_64-linux-gnu.so(+0x481c7) [0x7f0c886b11c7]\n#6 in /home/chris/miniconda3/envs/cuml/bin/python(+0x113339) [0x561e4e04e339]\n#7 in /home/chris/miniconda3/envs/cuml/bin/python: PyEval_EvalCode +0xa1 [0x561e4e1f5741]\n#8 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2d5ece) [0x561e4e210ece]\n#9 in /home/chris/miniconda3/envs/cuml/bin/python(+0x112f8e) [0x561e4e04df8e]\n#10 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2d099f) [0x561e4e20b99f]\n#11 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2d1c57) [0x561e4e20cc57]\n#12 in /home/chris/miniconda3/envs/cuml/bin/python(+0x113e38) [0x561e4e04ee38]\n#13 in /home/chris/miniconda3/envs/cuml/bin/python(+0x251adc) [0x561e4e18cadc]\n#14 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2515be) [0x561e4e18c5be]\n#15 in /home/chris/miniconda3/envs/cuml/bin/python: _PyObject_Call +0x12b [0x561e4e1701ab]\n#16 in /home/chris/miniconda3/envs/cuml/bin/python(+0x113339) [0x561e4e04e339]\n#17 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2d099f) [0x561e4e20b99f]\n#18 in /home/chris/miniconda3/envs/cuml/lib/python3.12/lib-dynload/_asyncio.cpython-312-x86_64-linux-gnu.so(+0x8274) [0x7f0e5f8de274]\n#19 in /home/chris/miniconda3/envs/cuml/lib/python3.12/lib-dynload/_asyncio.cpython-312-x86_64-linux-gnu.so(+0x8a63) [0x7f0e5f8dea63]\n#20 in /home/chris/miniconda3/envs/cuml/bin/python(+0x222fbc) [0x561e4e15dfbc]\n#21 in /home/chris/miniconda3/envs/cuml/bin/python(+0x34db0c) [0x561e4e288b0c]\n#22 in /home/chris/miniconda3/envs/cuml/bin/python(+0x1c402e) [0x561e4e0ff02e]\n#23 in /home/chris/miniconda3/envs/cuml/bin/python(+0x21940b) [0x561e4e15440b]\n#24 in /home/chris/miniconda3/envs/cuml/bin/python(+0x113339) [0x561e4e04e339]\n#25 in /home/chris/miniconda3/envs/cuml/bin/python: PyEval_EvalCode +0xa1 [0x561e4e1f5741]\n#26 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2d5ece) [0x561e4e210ece]\n#27 in /home/chris/miniconda3/envs/cuml/bin/python(+0x21940b) [0x561e4e15440b]\n#28 in /home/chris/miniconda3/envs/cuml/bin/python: PyObject_Vectorcall +0x2e [0x561e4e1541ae]\n#29 in /home/chris/miniconda3/envs/cuml/bin/python(+0x1126a1) [0x561e4e04d6a1]\n#30 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2eb328) [0x561e4e226328]\n#31 in /home/chris/miniconda3/envs/cuml/bin/python: Py_RunMain +0x3d1 [0x561e4e225ed1]\n#32 in /home/chris/miniconda3/envs/cuml/bin/python: Py_BytesMain +0x37 [0x561e4e1e00c7]\n#33 in /lib/x86_64-linux-gnu/libc.so.6(+0x29d90) [0x7f0e60d95d90]\n#34 in /lib/x86_64-linux-gnu/libc.so.6: __libc_start_main +0x80 [0x7f0e60d95e40]\n#35 in /home/chris/miniconda3/envs/cuml/bin/python(+0x2a4f71) [0x561e4e1dff71]\n"
     ]
    }
   ],
   "source": [
    "best_params, best_score = randomized_search_with_cv(\n",
    "    train_set, train_labels, param_distributions, n_iter=20, cv_method='kfold', k=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
