{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads csv and remove some data\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "# I am trying to optimize\n",
    "# BloodPressure of 0 is insane an means death - so values with 0 are equal to Null\n",
    "mean_bp = df[df[\"BloodPressure\"] != 0][\"BloodPressure\"].mean(\n",
    "    skipna=True,\n",
    ")\n",
    "mean_bp = round(mean_bp)\n",
    "\n",
    "df[\"BloodPressure\"] = df[\"BloodPressure\"].replace(0, mean_bp)\n",
    "\n",
    "# Same as for BloodPressure, 0 Insulin is nonsense. Not sure if mean is the right way to go\n",
    "# mean feels counter intuitive but i have no better idea at the moment\n",
    "mean_in = df[df[\"Insulin\"] != 0][\"Insulin\"].mean(skipna=True)\n",
    "mean_in = round(mean_in)\n",
    "\n",
    "df[\"Insulin\"] = df[\"Insulin\"].replace(0, mean_in)\n",
    "\n",
    "df = df.drop(columns=[\"SkinThickness\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in X and Y to separate into Features and Outcome. Than split each again in train and test.\n",
    "X = df.drop(\"Outcome\", axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializes the RandomForest (I have no idea what parameters work best or what arguments to change/tune,\n",
    "# there are a billion of them and i have no idea what to do, but i guess that's the exercise.)\n",
    "# Estimators and random_state is suggested by gpt\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "# Fits  evaluating the Model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=37)\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 250, 500],\n",
    "    # \"bootstrap\": [True, False], # False\n",
    "    # \"oob_score\": [], # Only without cross validation\n",
    "    \"max_depth\": [9, 10, 11],  # Around 10\n",
    "    \"min_samples_split\": [2, 3, 5, 6, 7, 8, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 3],  # Around 1\n",
    "    \"max_features\": [\"log2\", \"sqrt\"],  # Both viable\n",
    "    \"max_leaf_nodes\": [40, 50, 60],  # Around 50\n",
    "}\n",
    "\n",
    "rfr_grid = GridSearchCV(rfc, param_grid, cv=3, n_jobs=-1)\n",
    "rfr_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfr_grid.best_score_)\n",
    "results = pd.DataFrame(rfr_grid.cv_results_)\n",
    "print(results.info())\n",
    "print(results.describe())\n",
    "results = results.sort_values(by=[\"mean_test_score\"], ascending=False)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE some experimenting for fun\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "# I want to see if i can predict the missing values in BP and IN\n",
    "# I think this could result in better data and ultimately in a better model to predict diabetes\n",
    "# I start with BP\n",
    "\n",
    "df = df.drop(columns=[\"Insulin\"])\n",
    "df = df.drop(columns=[\"SkinThickness\"])\n",
    "\n",
    "data = df[df[\"BloodPressure\"] != 0]\n",
    "ToPredict = df[df[\"BloodPressure\"] == 0]\n",
    "\n",
    "\n",
    "X = data.drop(columns=[\"BloodPressure\"])\n",
    "y = data[\"BloodPressure\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializes the RandomForest (I have no idea what parameters work best or what arguments to change/tune,\n",
    "# there are a billion of them and i have no idea what to do, but i guess that's the exercise.)\n",
    "# Estimators and random_state is suggested by gpt\n",
    "clf = RandomForestRegressor(n_estimators=3000, random_state=42)\n",
    "\n",
    "# Fits  evaluating the Model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "def lenient_accuracy(y_true, y_pred):\n",
    "    \"\"\"Calculates the percentage of predictions within 5 points of the true value.\"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred) <= 5)\n",
    "\n",
    "\n",
    "# Calculate and print lenient accuracy\n",
    "lenient_score = lenient_accuracy(y_test, y_pred)\n",
    "print(f\"Lenient Accuracy: {lenient_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute differences between true and predicted values\n",
    "errors = np.abs(y_test - y_pred)\n",
    "\n",
    "# Get the errors for predictions OUTSIDE the lenient range\n",
    "large_errors = errors[errors > 5]\n",
    "\n",
    "# 1. Calculate the mean blood pressure\n",
    "mean_bp = np.mean(y_train)  # Calculate mean from training data\n",
    "\n",
    "# 2. Predict using the mean for all test instances\n",
    "mean_predictions = np.full_like(y_test, mean_bp)\n",
    "\n",
    "# 3. Calculate errors using the mean prediction\n",
    "mean_errors = np.abs(y_test - mean_predictions)\n",
    "\n",
    "# 4. Get errors outside the lenient range for the mean predictions\n",
    "mean_large_errors = mean_errors[mean_errors > 5]\n",
    "\n",
    "# 5. Analyze and compare\n",
    "print(\"Random Forest - Number of predictions outside lenient range:\", len(large_errors))\n",
    "print(\"Mean Prediction - Number of predictions outside lenient range:\", len(mean_large_errors))\n",
    "\n",
    "print(\"Random Forest - Average error for those predictions:\", np.mean(large_errors))\n",
    "print(\"Mean Prediction - Average error for those predictions:\", np.mean(mean_large_errors))\n",
    "\n",
    "# 6. (Optional) Visual comparison\n",
    "plt.hist(large_errors, bins=20, alpha=0.5, label=\"Random Forest Errors\")\n",
    "plt.hist(mean_large_errors, bins=20, alpha=0.5, label=\"Mean Prediction Errors\")\n",
    "plt.xlabel(\"Error\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Comparison of Errors Outside Lenient Range\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
